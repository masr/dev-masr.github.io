---
layout: post
title:  "两类感知器的一些说法"
date:   2014-05-22 23:12:00
category: geek
tags: ML algorhthm math neural-networks
---

<p class="excerpt">
<!--excerpt-->
最近看FudanNLP的论文，看到两类感知器的时候对其算法的原理蛮感兴趣。于是自己猜想了一下。
<!--excerpt-->
</p>

感知器是关于机器学习中的神经网络领域的，有关神经网络的原理我就不说了。主要讲两类感知器。

对于一个样本(x,y)，设$\phi(x)$为特征向量，y为类别标签，对于两类分类问题，可以令y=1或者y=-1。

于是我们需要一个映射函数$f(\phi(x))$，把特征向量$\phi(x)$映射为一个二元值{1,-1}。

我们可以令~f(\phi(x))={(1,if \phi(x)\*w+b>0),(-1,else):}~

w和权重变量，b为偏移变量。这两个变量就是我们需要训练和求解的参数。

FudanNlp的论文上说，感知器算法通过多次迭代来更新参数w。先选取一个w的初始值，每轮迭代t中,从样本集中选取样本$x_t$,
如果分类正确,则w参数不作修改,如果分类错误,这通过下面公式更新w

$w_t=w_{t-1}+\alpha y_t\phi(x_t)$

其中$\alpha$是一个大于0点的常数，比如说是0.2。那么这个公式是什么意思呢？为什么最后就可以训练出w呢？

我也是百思不得其解，后来终于想到了一个说法。

先假设，$y_t=1$，但是我们的估计值是-1，那么这个时候就要更新w，令$w=w+\alpha\phi(x)$，
其实从这个角度来说，w是更加接近$\phi(x)$了，为什么呢？假设$\alpha$很大,有10000，那么$w=w+10000\phi(x)。

此时~w\\~\\~\phi(x)~ ，~w\*\phi(x)\\~\\~\phi^2(x)>0~，所以其结果就趋向于1，也就是正确的分类标签。

如果$y_t=-1$，但是我们的估计值是1，那么还是要更新w，令$w=w-\alpha\phi(x)$，那么w肯定是更加接近于$\phi(x)$的负值。
也就是~w\\~\\~-\phi(x)~，~w\*\phi(x)\\~\\~\-phi^2(x)&lt;0~，所以其结果就趋向于-1，也就是正确的分类标签。

所以经过不断地迭代，不断地调整，最终会趋向于一个良好的参数。

